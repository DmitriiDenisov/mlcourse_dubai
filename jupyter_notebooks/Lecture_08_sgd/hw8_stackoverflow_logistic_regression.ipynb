{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Homework \\#8\n",
    "## <center> Logistic regression for tagging questions from StackOverflow\n",
    "\n",
    "**You have to infer some formulas where it is needed (yeah, you will need pen and paper), also you have to fill code where is is asked.**\n",
    "\n",
    "## 0. Description\n",
    "\n",
    "In this homework, we will study and program a model for predicting tags in the text of a question based on multiclass logistic regression. Unlike the usual statement of the classification problem (multiclass), in this case, one example can belong to several classes (multilabel) at the same time. We will implement the online version of the multilabel classification algorithm.\n",
    "\n",
    "We will use a small sample of tagged questions from the StackOverflow website with a size of 125 thousand examples (find in the `data/hw8` folder).\n",
    "\n",
    "\n",
    "PS: It can be shown that such an implementation is not at all effective and it would be easier to use vectorized calculations. For this dataset it is. But actually, such implementation is used in real life, but naturally, they are not written in Python. For example, a banner is shown to the user in the [CTR](https://en.wikipedia.org/wiki/Click-through_rate) forecasting online models, then, depending on the presence of a click, the model parameters are updated. In real life, the parameters of the model can be several hundred million. But usually, in such sparse data, not none parameters belonging to the user would be less than 100k. Often all this is stored in huge clusters in in-memory databases, and user processing is distributed.\n",
    "\n",
    "PS2:\n",
    "- in the process of solving homework, you will have to work with the text, and you may want to do obvious preprocessing, for example, put all the words in lower case, but  **you do not need to do this unless you are asked to do this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "\n",
    "# change it to your location\n",
    "DS_FILE_NAME = './../../data/hw8/stackoverflow_sample_125k.tsv.gz'\n",
    "TAGS_FILE_NAME = './../../data/hw8/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'html', 'javascript', 'php', 'c#', 'jquery', 'android', 'java', 'python', 'ios', 'c++'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiclass and Multilabel Logistic Regression\n",
    "\n",
    "\n",
    "Let's recap what is the Logistic Regression for binary classification $\\left\\{0, 1\\right\\}$. Probability that a sample belongs to a class $1$ can be inferred using [Bayes Formula](https://en.wikipedia.org/wiki/Bayes%27_theorem):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "where:\n",
    "- $\\vec{x}$ – features of the sample\n",
    "- $\\sigma$ – logistic sigmoid function\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – log odds is modelled by a linear function\n",
    "\n",
    "We can generalize it to a set of $K$ classes, only denominator will be changed in the Bayes Formula. Let's write down a probability of belonging to a class $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – a [softmax function](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – now instead of one line which separates two classes, we have several lines for each class $k$ which separates it from all others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [categorical disribution](https://en.wikipedia.org/wiki/Categorical_distribution) for a likelihood function, or it's log:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "This is well known [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) function (after multiplication to $-1$). Usually we maximize likelihood, but in the case of cross entropy we minimize it (because cross entropy is a measure between two distributions, and we want to move closer model distribution to a ground truth). To infer rule of updating weights we have to take a derivative wrt parameters, **plz do it if you have never done it before** (here is the solution [one](https://www.ics.uci.edu/~pjsadows/notes.pdf) and [two](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Vector of softmax values $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ forms a discrete probability distribution, i.e. $\\sum_{i=1}^K \\sigma_i = 1$. But in our setup each sample may have one or more tags, or belongs to one or more classes. We modify the model:\n",
    "- let's consider that each tag is independant from others, i.e. for each outcome we use binary logistic regression (either sample has the tag or not); then we can write down probabilities of a tag of the sample:\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- like in LR, presence of a tag we model by <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">Bernoulli distribution</a>\n",
    "\n",
    "<font color=\"red\">Question 1.</font> Your first task – is to infer and write down simplified loss function, which is negative log of likelihood of features of a sample $\\vec{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Options:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Update rule inference\n",
    "\n",
    "<font color=\"red\">Question 2.</font>Infer and write down an updating rule of parameters of the model, taking derivative of $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Options:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of the base model\n",
    "\n",
    "### Shortest introduction to text processing\n",
    "\n",
    "<img src='./../../img/Bag-Of-Words-BOW-representation-of-the-input-that-is-next-follow-by-Softmax.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the base class\n",
    "\n",
    "In this exercise, you have a facade of the class, study it carefully, pay attention to all comments in the code. Then plz fill gaps in the code where is it asked.\n",
    "\n",
    "As you may notice from the code, for updating parameter $w_{km}$ we use only feature $x_m$, which equals to $0$ if there is now word with index $m$ in the sentence, and greater then $0$ if there is such word. In our case, if we don't want to recalculate [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) for the incoming sentence, we are iterating over words of the sentence in the order of the sentence. If a word occurs several times, then we add it to the cummolative variable with its weight. **When you accumulate linear combination of words and parameters in $z$, you should count only non-zero features of an object.**\n",
    "\n",
    "Hint:\n",
    "- if you implement calculation of the sigmoid function in a straightforward way like in formula, then big negative $z$ will give you large value of $e^{-z}$ which may exceed limits of `float32`\n",
    "- meanwhile $e^{-z}$ of big positive $z$ will be zero\n",
    "- use properties of $\\sigma$ to fix this numerical issue, to avoid overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Constructor of the class\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        list of all tags\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # dictionary which conains mapping from words to indices (to save memory)\n",
    "        # e.g. self._vocab['exception'] = 17 means that word `exception` has index equal to 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # parameters of the model / weights\n",
    "        # for each class/tag we have to store its own vector of weights\n",
    "        # by default all weights equal to zero\n",
    "        # in advance we don't know total number of parameters, because we don't know the size of the vacabulary\n",
    "        # so for each class we have dictionary wich variable number of keys stored there\n",
    "        # e.g. self._w['java'][self._vocab['exception']]  is a weight of word `exception` for tag `java`\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # parameter of the model: w_0 or bias\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"One iteration over dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        file name with the data\n",
    "        \n",
    "    top_n_train : int\n",
    "        first top_n_train lines will be used for training, the rest for testing\n",
    "        \n",
    "    total : int, default=125000\n",
    "        total number of line, used only to show progress par\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        learning rate of gradient decsent algorithm\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        tolerance of log\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # open gzipped text file\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            \n",
    "            # iterate over lines in the file\n",
    "            for line in tqdm(f, total=total, mininterval=1):\n",
    "                pair = line.decode().strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # words of the sentence, these are features x\n",
    "                sentence = sentence.split(' ')\n",
    "                # tags of the sentence, these are targets y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # loss of the current sample\n",
    "                sample_loss = 0\n",
    "\n",
    "                # calculate gradients and update weights\n",
    "                # iterate over tags, because each tag has its own list of parameters\n",
    "                for tag in self._tags:\n",
    "                    # target variable is 1 if a tag is in the set of tags of the current sentence\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # calculation of the argument of sigmoind\n",
    "                    # linear combination of weights and features\n",
    "                    # here we initialize z with some value\n",
    "                    # FILL GAP IN THE CODE\n",
    "                    # z = ...\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # if in the testing mode we have word which was not seen in the training part\n",
    "                        # we ignore it\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        # update z\n",
    "                        # FILL GAP IN THE CODE\n",
    "                        # z += ...\n",
    "    \n",
    "                    # calculate value of the sigmoid function\n",
    "                    # FILL GAP IN THE CODE\n",
    "                    # sigma = ...\n",
    "    \n",
    "                    \n",
    "                    # update smaple loss value\n",
    "                    # FILL GAP IN THE CODE\n",
    "                    # sample_loss += ...\n",
    "                 \n",
    "                    \n",
    "                    # if we are in the training part of the dataset, we update weights\n",
    "                    if n < top_n_train:\n",
    "                        # calculate derivative of the loss function wrt parameter\n",
    "                        # FILL GAP IN THE CODE\n",
    "                        # dLdw = ...\n",
    "\n",
    "                        # update weights\n",
    "                        # we minimize negatime log likelihood (the second minus sign)\n",
    "                        # why we follow negative direction of the gradient (the first minus sign)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's create an instance of the object and iterate through dataset \n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the value of the negative log likelihood really decreased. Since we use stochastic gradient descent, we should not expect a smooth fall in the error function. We will use a moving average with a window of 10 thousand examples to somehow smooth out the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Question 3.</font>\n",
    "Calculate mean value of the loss function on the last 10 000 examples of testing set. Choose the closest answer to yours from he following list.\n",
    "\n",
    "<font color=\"red\">Options:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "In the base model we used first 100 000 line for training, and the rest for testing. As you may see, value of negative log likelihood is not very informative. You have to modify the base model, such that `iterate_file` would return value of _accuracy_ on the test set. \n",
    "\n",
    "We define accuracy as:\n",
    "- we consider a threshold equal to `0.9`, if the predicted probability of a tag is greater then the threshold, we consder that the input sentence has the tag\n",
    "- we calculate accuracy using [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) between the set of predicted tags and the set of ground truth tags\n",
    "  - e.g. if a sentence has two tags ['html', 'jquery'], but model predicted three ['ios', 'html', 'java'], then Jaccard index equals to |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- method `iterate_file` returns **average** accuracy on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE LogRegressor CLASS\n",
    "# Put the new version of the class here\n",
    "# FILL GAP IN THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# print accuracy\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Question 4.</font> Find the closest answer to the printed accuracy from the following list?\n",
    "\n",
    "<font color=\"red\">Options:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. $L_2$-regilarization\n",
    "\n",
    "Your fifth task is to upgrade the model `LogRegressor` such that it would support $L_2$-regularization. In the method `iterate_file` should be rarameter `lmbda=0.01` with the default value. Takin into account regilarization, new loss function wil be:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Gradient of the first term we have already inferred, for the second one it is:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "If we would regularize ALL weights for each sentence we will slow down the process dramatically, because we will need to iterate over vacabulary for each sentence. But we will use dirty trick from real life, which would drop quality a bit, but boost computational performace: we will apply rerularization only for such parameters which correspondant words are presented in the sentence. Don't forget, that we don't regularize bias/w_0. Also do not modify `sample_loss`, calculate it without regularization.\n",
    "\n",
    "Hint:\n",
    "- don't forget, that you need to regularize parameter once, even if there were several correspondant words\n",
    "- let's agree to regularize a parameter, only if we see its word for a first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE LogRegressor CLASS\n",
    "# Put the new version of the class here\n",
    "# FILL GAP IN THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Question 5.</font> Find the closest answer to the printed accuracy from the following list?\n",
    "\n",
    "<font color=\"red\">Options:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet Regularization, formulas\n",
    "Beside $L_2$ regularization, $L_1$ is used quite often.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "ElasticNet is a linear combination of $L_1$ and $L_2$:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- where $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "<font color=\"red\">Question 6.</font> Find a gradient of ElasticNet (do not take into account $-\\mathcal{L}$ for now). \n",
    "\n",
    "<font color=\"red\">Options:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet Regularization, implementation\n",
    "\n",
    "Please update `LogRegressor` so that it would support `ElasticNet`. Method `iterate_file` now should take `lmbda=0.0002` and `gamma=0.1`. Run one pass over dataset with `ElasticNet` and default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATE LogRegressor CLASS\n",
    "# Put the new version of the class here\n",
    "# FILL GAP IN THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Question 7.</font> Find the closest answer to the printed accuracy from the following list?\n",
    "\n",
    "<font color=\"red\">Options:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Most important words for a tag\n",
    "\n",
    "Linear models are easy to interpretate. Please find the most important words for each tag and answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Question 8.</font> For many tags, the presence of the tag itself in the sentence is an important signal, for them the tag itself is the strongest signal, which is not surprising. For which of the tags is the set of tags, the name of the tag itself is not in the top 5 most important?\n",
    "\n",
    "<font color=\"red\">Options:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reduce the size of the vacabulary\n",
    "Currently we have 519290 words in the vacabulary. If it would be full StackOverflow dump, there would be 10 millions of words. We can regularize models not only using mathematics (e.g. `L1` and `L2`), but using empirical knowledge. For example we know that there are many stop words, emojis and just non informative words. So let's reduce ste size of the vacabulary manually.\n",
    "\n",
    "Your job is to modify `LogRegressor`:\n",
    "- add to the method `iterate_file` one more argument with default value `update_vocab=True`\n",
    "- with `update_vocab=True` model can add new words to the vacabulary\n",
    "- with `update_vocab=False` model should ignore all words which are not presented in the vacabulary\n",
    "- add method `filter_vocab(n=10000)`, which would keep only top-n most popular words using data from `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATE LogRegressor CLASS\n",
    "# Put the new version of the class here\n",
    "# FILL GAP IN THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's keep only 10 000 words\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do one more pass over dataset, with reduced learning rate\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Question 9.</font> Find the closest answer to the printed accuracy from the following list? \n",
    "\n",
    "<font color=\"red\">Options:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predic tags for a new question\n",
    "\n",
    "The last task is to write a function `predict_proba`, which take a string as input, returns a list of predicted tags with their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATE LogRegressor CLASS\n",
    "# Put the new version of the class here\n",
    "# FILL GAP IN THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Choose all tags, using threshold equal to $0.9$. \n",
    "\n",
    "<font color=\"red\">Options (multiple):</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
